{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  2 15:31:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P0    12W /  N/A |    932MiB /  4096MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1604    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      2828    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      6180    C+G   ...\\app-1.0.9004\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A      6876    C+G   ...erver\\YourPhoneServer.exe    N/A      |\n",
      "|    0   N/A  N/A      8492    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13996    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     16524    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     19108    C+G   ... Host\\Razer Synapse 3.exe    N/A      |\n",
      "|    0   N/A  N/A     19836    C+G   ...arp.BrowserSubprocess.exe    N/A      |\n",
      "|    0   N/A  N/A     21440    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21444    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     24316    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     24704    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     25424    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     29280    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     33544    C+G   ...\\LeagueClientUxRender.exe    N/A      |\n",
      "|    0   N/A  N/A     42236    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     43732    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     46668    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "# @title Check if cuda is available for your gpu\n",
    "import torch\n",
    "print(\"\\n\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Random\\Anaconda\\lib\\site-packages\\pytorch_lightning\\metrics\\__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n",
      "D:\\Random\\Anaconda\\lib\\site-packages\\kornia\\augmentation\\augmentation.py:1830: DeprecationWarning: GaussianBlur is no longer maintained and will be removed from the future versions. Please use RandomGaussianBlur instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# @title Load libraries and variables\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "sys.path.insert(1, '/content/taming-transformers')\n",
    "from IPython import display\n",
    "from base64 import b64encode\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from taming.models import cond_transformer, vqgan\n",
    "import taming.modules \n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from CLIP import clip\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import imageio\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.view([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.view([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "\n",
    "\n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    "\n",
    "\n",
    "replace_grad = ReplaceGrad.apply\n",
    "\n",
    "\n",
    "class ClampWithGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, min, max):\n",
    "        ctx.min = min\n",
    "        ctx.max = max\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min, max)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
    "\n",
    "\n",
    "clamp_with_grad = ClampWithGrad.apply\n",
    "\n",
    "\n",
    "def vector_quantize(x, codebook):\n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)\n",
    "\n",
    "\n",
    "class Prompt(nn.Module):\n",
    "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('weight', torch.as_tensor(weight))\n",
    "        self.register_buffer('stop', torch.as_tensor(stop))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
    "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        dists = dists * self.weight.sign()\n",
    "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
    "\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    vals = prompt.rsplit(':', 2)\n",
    "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
    "    return vals[0], float(vals[1]), float(vals[2])\n",
    "\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "        self.augs = nn.Sequential(\n",
    "            # K.RandomHorizontalFlip(p=0.5),\n",
    "            # K.RandomVerticalFlip(p=0.5),\n",
    "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
    "            # K.RandomSharpness(0.3,p=0.4),\n",
    "            # K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5),\n",
    "            # K.RandomCrop(size=(self.cut_size,self.cut_size), p=0.5),\n",
    "            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
    "            K.RandomPerspective(0.7,p=0.7),\n",
    "            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
    "            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),\n",
    "            \n",
    ")\n",
    "        self.noise_fac = 0.1\n",
    "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        \n",
    "        for _ in range(self.cutn):\n",
    "\n",
    "            # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            # offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            # offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            # cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            # cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "\n",
    "            # cutout = transforms.Resize(size=(self.cut_size, self.cut_size))(input)\n",
    "            \n",
    "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
    "            cutouts.append(cutout)\n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "def load_vqgan_model(config_path, checkpoint_path):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
    "        model = vqgan.VQModel(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
    "        model = vqgan.GumbelVQ(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
    "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
    "        parent_model.eval().requires_grad_(False)\n",
    "        parent_model.init_from_ckpt(checkpoint_path)\n",
    "        model = parent_model.first_stage_model\n",
    "    else:\n",
    "        raise ValueError(f'unknown model type: {config.model.target}')\n",
    "    del model.loss\n",
    "    return model\n",
    "\n",
    "\n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Parameters\n",
    "texts = \"orion cybernetic city\" #@param {type:\"string\"}\n",
    "width =  480#@param {type:\"number\"}\n",
    "height = 480#@param {type:\"number\"}\n",
    "model = \"vqgan_imagenet_f16_16384\" #@param [\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"vqgan_openimages_f16_8192\", \"wikiart_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\"]\n",
    "images_interval =  20#@param {type:\"number\"}\n",
    "init_image = \"\"#@param {type:\"string\"}\n",
    "target_images = \"\"#@param {type:\"string\"}\n",
    "seed = -1#@param {type:\"number\"}\n",
    "max_iterations = 500#@param {type:\"number\"}\n",
    "\n",
    "model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", 'vqgan_openimages_f16_8192':'OpenImages 8912',\n",
    "                 \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\"}\n",
    "name_model = model_names[model]     \n",
    "\n",
    "if seed == -1:\n",
    "    seed = None\n",
    "if init_image == \"None\":\n",
    "    init_image = None\n",
    "if target_images == \"None\" or not target_images:\n",
    "    target_images = []\n",
    "else:\n",
    "    target_images = target_images.split(\"|\")\n",
    "    target_images = [image.strip() for image in target_images]\n",
    "\n",
    "texts = [phrase.strip() for phrase in texts.split(\"|\")]\n",
    "if texts == ['']:\n",
    "    texts = []\n",
    "\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    prompts=texts,\n",
    "    image_prompts=target_images,\n",
    "    noise_prompt_seeds=[],\n",
    "    noise_prompt_weights=[],\n",
    "    size=[width, height],\n",
    "    init_image=init_image,\n",
    "    init_weight=0.,\n",
    "    clip_model='ViT-B/32',\n",
    "    vqgan_config=f'{model}.yaml',\n",
    "    vqgan_checkpoint=f'{model}.ckpt',\n",
    "    step_size=0.1,\n",
    "    cutn=32,\n",
    "    cut_pow=1.,\n",
    "    display_freq=images_interval,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using texts: ['orion cybernetic city']\n",
      "Using seed: 173533237245600\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips\\vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Restored from vqgan_imagenet_f16_16384.ckpt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"softmax_lastdim_kernel_impl\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18312/1665705792.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_prompt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperceptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mpMs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPrompt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaom\\OneDrive\\Ambiente de Trabalho\\Image Generation\\CLIP\\clip\\model.py\u001b[0m in \u001b[0;36mencode_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# NLD -> LND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# LND -> NLD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_final\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaom\\OneDrive\\Ambiente de Trabalho\\Image Generation\\CLIP\\clip\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaom\\OneDrive\\Ambiente de Trabalho\\Image Generation\\CLIP\\clip\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaom\\OneDrive\\Ambiente de Trabalho\\Image Generation\\CLIP\\clip\\model.py\u001b[0m in \u001b[0;36mattention\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[0;32m   1029\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[0;32m   1030\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[0;32m   1032\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[0;32m   5080\u001b[0m     \u001b[1;31m# (deep breath) calculate attention and out projection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5081\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5082\u001b[1;33m     \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5083\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5084\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[0;32m   4826\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4827\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4828\u001b[1;33m     \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4829\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4830\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Random\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1677\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1678\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1679\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1680\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1681\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"softmax_lastdim_kernel_impl\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "#@title Actually do the run...\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# del variables\n",
    "# gc.collect()\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if texts:\n",
    "    print('Using texts:', texts)\n",
    "if target_images:\n",
    "    print('Using image prompts:', target_images)\n",
    "if args.seed is None:\n",
    "    seed = torch.seed()\n",
    "else:\n",
    "    seed = args.seed\n",
    "torch.manual_seed(seed)\n",
    "print('Using seed:', seed)\n",
    "\n",
    "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
    "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "# clock=deepcopy(perceptor.visual.positional_embedding.data)\n",
    "# perceptor.visual.positional_embedding.data = clock/clock.max()\n",
    "# perceptor.visual.positional_embedding.data=clamp_with_grad(clock,0,1)\n",
    "\n",
    "cut_size = perceptor.visual.input_resolution\n",
    "\n",
    "f = 2**(model.decoder.num_resolutions - 1)\n",
    "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
    "\n",
    "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
    "sideX, sideY = toksX * f, toksY * f\n",
    "\n",
    "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
    "    e_dim = 256\n",
    "    n_toks = model.quantize.n_embed\n",
    "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
    "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
    "else:\n",
    "    e_dim = model.quantize.e_dim\n",
    "    n_toks = model.quantize.n_e\n",
    "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
    "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
    "# z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
    "# z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
    "\n",
    "# normalize_imagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                            std=[0.229, 0.224, 0.225])\n",
    "\n",
    "if args.init_image:\n",
    "    if 'http' in args.init_image:\n",
    "      img = Image.open(urlopen(args.init_image))\n",
    "    else:\n",
    "      img = Image.open(args.init_image)\n",
    "    pil_image = img.convert('RGB')\n",
    "    pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
    "    pil_tensor = TF.to_tensor(pil_image)\n",
    "    z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n",
    "else:\n",
    "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
    "    # z = one_hot @ model.quantize.embedding.weight\n",
    "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
    "        z = one_hot @ model.quantize.embed.weight\n",
    "    else:\n",
    "        z = one_hot @ model.quantize.embedding.weight\n",
    "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n",
    "    z = torch.rand_like(z)*2\n",
    "z_orig = z.clone()\n",
    "z.requires_grad_(True)\n",
    "opt = optim.Adam([z], lr=args.step_size)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "\n",
    "\n",
    "pMs = []\n",
    "\n",
    "for prompt in args.prompts:\n",
    "    txt, weight, stop = parse_prompt(prompt)\n",
    "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
    "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
    "\n",
    "for prompt in args.image_prompts:\n",
    "    path, weight, stop = parse_prompt(prompt)\n",
    "    img = Image.open(path)\n",
    "    pil_image = img.convert('RGB')\n",
    "    img = resize_image(pil_image, (sideX, sideY))\n",
    "    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
    "    embed = perceptor.encode_image(normalize(batch)).float()\n",
    "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
    "\n",
    "for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
    "    gen = torch.Generator().manual_seed(seed)\n",
    "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
    "    pMs.append(Prompt(embed, weight).to(device))\n",
    "\n",
    "def synth(z):\n",
    "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
    "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
    "    else:\n",
    "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
    "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def checkin(i, losses):\n",
    "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
    "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
    "    out = synth(z)\n",
    "    TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
    "    display.display(display.Image('progress.png'))\n",
    "\n",
    "def ascend_txt():\n",
    "    global i\n",
    "    out = synth(z)\n",
    "    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    if args.init_weight:\n",
    "        # result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
    "        result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1/torch.tensor(i*2 + 1))*args.init_weight) / 2)\n",
    "    for prompt in pMs:\n",
    "        result.append(prompt(iii))\n",
    "    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    imageio.imwrite('./steps/' + str(i) + '.png', np.array(img))\n",
    "\n",
    "    return result\n",
    "\n",
    "def train(i):\n",
    "    opt.zero_grad()\n",
    "    lossAll = ascend_txt()\n",
    "    if i % args.display_freq == 0:\n",
    "        checkin(i, lossAll)\n",
    "       \n",
    "    loss = sum(lossAll)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    with torch.no_grad():\n",
    "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
    "\n",
    "i = 0\n",
    "try:\n",
    "    with tqdm() as pbar:\n",
    "        while True:\n",
    "            train(i)\n",
    "            if i == max_iterations:\n",
    "                break\n",
    "            i += 1\n",
    "            pbar.update()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate a video with the result\n",
    "\n",
    "init_frame = 1 #This is the frame where the video will start\n",
    "last_frame = i #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
    "\n",
    "min_fps = 10\n",
    "max_fps = 60\n",
    "\n",
    "total_frames = last_frame-init_frame\n",
    "\n",
    "length = 15 #Desired time of the video in seconds\n",
    "\n",
    "frames = []\n",
    "tqdm.write('Generating video...')\n",
    "for i in range(init_frame,last_frame): #\n",
    "    frames.append(Image.open(\"./steps/\"+ str(i) +'.png'))\n",
    "\n",
    "#fps = last_frame/10\n",
    "fps = np.clip(total_frames/length,min_fps,max_fps)\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
    "for im in tqdm(frames):\n",
    "    im.save(p.stdin, 'PNG')\n",
    "p.stdin.close()\n",
    "p.wait()\n",
    "mp4 = open('video.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "display.HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02546cf1e4e9723e0e75de43c12158ca41fff6eb94d9ab4ee9edb8b154474350"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
